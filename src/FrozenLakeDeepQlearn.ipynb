{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents:\n",
    "* [Q Learning for Frozen Lake v0](#Qtable)\n",
    "* [Deep Q Learning for Frozen Lake v0](#DeepQ)\n",
    "* [Version with ptan](#Ptan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Q Learning for Frozen Lake v0 <a class=\"anchor\" id=\"Qtable\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm\n",
    "\n",
    "- **initialize** Hyperparameters, Env\n",
    "- **Repeat** for certain number of episodes:\n",
    "    - **Repeat** for maximum steps in one Episode:\n",
    "        - do a epsilon greedy step\n",
    "            - with probability $\\epsilon$ choose $a$ as 1 of 4 possible random actions <br>\n",
    "            otherwise $a=\\underset{a'}{\\operatorname{argmax}}Q(s,a')$\n",
    "        - Update Q values according to Bellman Equation:<br>\n",
    "        $Q(s,a)=Q(s,a)+\\mathrm{learning\\_rate}\\cdot\\left(R(s,a)+\\gamma\\cdot\\max\\limits_{a'}Q(s',a')-Q(s,a)\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import sys\n",
    "%matplotlib notebook\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "\n",
    "total_episodes = 20000       # Total episodes\n",
    "learning_rate = 0.7          # Learning rate\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0                 # Exploration rate\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.01            # Minimum exploration probability\n",
    "decay_rate = 0.005            # Exponential decay rate for exploration prob\n",
    "\n",
    "directions = {0:'left', 1:'down', 2:'right', 3:'up'}\n",
    "\n",
    "rewards = []\n",
    "for episode in range(total_episodes):\n",
    "    if (episode % (total_episodes/100)) == 0:\n",
    "        print('{} %'.format(round(episode/total_episodes*100)), end='\\r')\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    reward_tot = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Epsilon Greedy action choice\n",
    "        rand = np.random.rand()\n",
    "        if rand > epsilon:\n",
    "            action = np.argmax(qtable[state])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Do an action and update Qtable afterwards\n",
    "        state_n, reward, done,_ = env.step(action)\n",
    "\n",
    "        qtable[state,action] = qtable[state,action] + learning_rate * (reward + gamma*np.max(qtable[state_n]) - qtable[state,action])\n",
    "\n",
    "        state = state_n\n",
    "\n",
    "        if done == True:\n",
    "            reward_tot = reward\n",
    "            break;\n",
    "\n",
    "    # make epsilon smaller exponentially and append reward of episode to list of all rewards\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    rewards.append(reward_tot)\n",
    "\n",
    "print('{} %'.format(100), end='\\r')\n",
    "\n",
    "#np.save('Data/qtable', qtable)\n",
    "print(\"\\nMean reward: \" + str(sum(rewards[-100:])/100))\n",
    "\n",
    "#qtable = np.load(\"Data/qtable.npy\")\n",
    "\n",
    "#print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def testQTable(output):\n",
    "    ''' Test Q table and print out average of 100 episodes (if output == False)\n",
    "    or print one trajectory into file (if output == True)\n",
    "    '''\n",
    "    step_count = 0\n",
    "    if output == True:\n",
    "        standard_out = sys.stdout\n",
    "        sys.stdout = open('Render.txt', 'w')\n",
    "\n",
    "        tries = 1\n",
    "    else:\n",
    "        tries = 100\n",
    "    mean_reward = 0\n",
    "    for i in range(tries):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        tot_reward = 0\n",
    "        if output == True:\n",
    "            env.render()\n",
    "        for step in range(max_steps):\n",
    "            action = np.argmax(qtable[state])\n",
    "            state_n, reward, done, info = env.step(action)\n",
    "            tot_reward += reward\n",
    "            if output == True:\n",
    "                env.render()\n",
    "            state = state_n\n",
    "\n",
    "            if done == True:\n",
    "                step_count = step+1\n",
    "                mean_reward += tot_reward\n",
    "                break\n",
    "\n",
    "    mean_reward /= tries\n",
    "    if output == True:\n",
    "        sys.stdout = standard_out\n",
    "        print('Reward: {} after {} steps'.format(mean_reward, step_count))\n",
    "    else:\n",
    "        print('Average reward: {}'.format(mean_reward))\n",
    "\n",
    "    env.reset()\n",
    "    env.close()\n",
    "\n",
    "testQTable(output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Deep Q Learning for Frozen Lake v0 <a class=\"anchor\" id=\"DeepQ\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm\n",
    "\n",
    "- **initialize** Hyperparameters, Agent/Env, DQN and Target DQN \n",
    "- **Transform** Observations to one hot encoding: e.g. **3 -> [0,0,0,1,0,...,0]**\n",
    "- **Repeat** until mean reward of last 100 games has exceeded reward-bound:\n",
    "    - do a epsilon greedy step\n",
    "        - with probability $\\epsilon$ choose $a$ as 1 of 4 possible random actions <br>\n",
    "        otherwise $a=\\underset{a'}{\\operatorname{argmax}}Q(s,a'|\\theta)$\n",
    "    - put old state $s$, action $a$, reward $r$, done flag, new state $s'$ in experience buffer\n",
    "    - if episode is done:<br>\n",
    "        - reset Agent/Env\n",
    "    - if length of buffer is smaller than certain limit\n",
    "        - do nothing and repeat loop\n",
    "    - if current step is multiple of some certain synchronization number\n",
    "        - updates target nets weights with net weights <br>\n",
    "        $\\hat{\\theta}\\leftarrow\\theta$\n",
    "    - Sample batch of batch size 100\n",
    "    - calculate loss of sampled batch btw. net and target net:\n",
    "        - Loss $L = \\left(Q\\left(s,a|\\theta\\right)-R(s,a)+\\gamma\\cdot\\max\\limits_{a'}\\hat{Q}\\left(s',a'|\\hat{\\theta}\\right)\\right)^2$ <br>\n",
    "        or $L = \\left(Q\\left(s,a|\\theta\\right)-R(s,a)\\right)^2$ if $s'$ is final state\n",
    "    - backpropagate and update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "DEFAULT_ENV_NAME = \"FrozenLake-v0\"\n",
    "MEAN_REWARD_BOUND = 0.6\n",
    "HIDDEN_LAYER = 128\n",
    "GAMMA = 0.95\n",
    "# Higher batch size to have at least one succsessfull sample for training\n",
    "BATCH_SIZE = 100\n",
    "BUFFER_SIZE = 50000\n",
    "# Lower learning rate to average more samples\n",
    "LEARNING_RATE = 0.001\n",
    "SYNC_TARGET_FRAMES = 100\n",
    "REPLAY_START_SIZE = 2000\n",
    "\n",
    "EPSILON_STEPS = 5000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.02\n",
    "\n",
    "'''\n",
    "Defining the Deep NN\n",
    "'''\n",
    "class DQN(nn.Module):\n",
    "    ''' Define Deep Neural Network of form input -> hidden -> output\n",
    "    '''\n",
    "    def __init__(self, input_nr, action_nr):\n",
    "        nn.Module.__init__(self)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_nr, HIDDEN_LAYER),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_LAYER, action_nr)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x.float())\n",
    "\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    ''' Wrap observed state in one hot encoding\n",
    "    '''\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        ''' get number 1-15  as observation and make it to float array\n",
    "        '''\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "\n",
    "#env = gym.wrappers.Monitor(DiscreteOneHotWrapper(gym.make(DEFAULT_ENV_NAME)),'TestMonitor')\n",
    "env = DiscreteOneHotWrapper(gym.make(DEFAULT_ENV_NAME))\n",
    "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', 'done', 'new_state'])\n",
    "\n",
    "\n",
    "class ExperienceBuffer:\n",
    "    ''' Experience Buffer class with buffer as collections.deque object with length of given capacity\n",
    "    methods:\n",
    "        __init__:   initialize the buffer\n",
    "        __len__:    return the length\n",
    "        append:     append one given experience and append it on the right\n",
    "        sample:     sample mini batch from stored experiences and return them as seperated numpy arrays\n",
    "                    also: delete sampled items (?)\n",
    "    '''\n",
    "    # Buffer to store the experiences to learn from\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        # queue one element into the buffer\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # sample batch_size elements from the buffer and return the experience\n",
    "        # as numpy arrays\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        \n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), \\\n",
    "               np.array(dones, dtype=np.uint8), np.array(next_states)\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    ''' Agent class with environment and experience buffer\n",
    "    methods:\n",
    "        __init__:   initialize of environment & buffer\n",
    "        _reset:     reset of environment and render state if specified (render==True)\n",
    "        play_step:  make an epsilon greedy step and store it in the experience buffer as specified in text\n",
    "                    above the code\n",
    "                    return reward if episode is done else return None\n",
    "        close:      close the environment\n",
    "    '''\n",
    "    def __init__(self, env, exp_buffer, render=False):\n",
    "        self.env = env\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset(render)\n",
    "\n",
    "    #def state_to_one_hot(self):\n",
    "    #    self.state = F.one_hot(torch.tensor(self.state), num_classes=16)\n",
    "\n",
    "    def _reset(self, render=False):\n",
    "        self.state = self.env.reset()\n",
    "        if render == True:\n",
    "            self.env.render()\n",
    "\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\", render=False):\n",
    "        done_reward = None\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([self.state], copy=False)\n",
    "            state_v = torch.tensor(state_a)\n",
    "            q_vals_v = net(state_v)\n",
    "            act_v = torch.max(q_vals_v, dim=1)[1]\n",
    "            action = act_v.item()\n",
    "\n",
    "        # do step in the environment\n",
    "        new_state, reward, is_done, _ = self.env.step(action)\n",
    "        if render == True:\n",
    "            self.env.render()\n",
    "\n",
    "        exp = Experience(self.state, action, reward, is_done, new_state)\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if is_done:\n",
    "            done_reward = reward\n",
    "            self._reset()\n",
    "        return done_reward\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
    "    ''' Calculate loss according to description above this code\n",
    "    '''\n",
    "    states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "    states_v = torch.tensor(states)\n",
    "    next_states_v = torch.tensor(next_states)\n",
    "    actions_v = torch.tensor(actions)\n",
    "    rewards_v = torch.tensor(rewards)\n",
    "    done_mask = torch.ByteTensor(dones)\n",
    "\n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1).long()).squeeze(-1) #.long() for windows?\n",
    "    ##### ALTERNATIVE loop\n",
    "    #mat = net(states_v)\n",
    "    #state_action_values = torch.zeros(mat.shape[0])\n",
    "    #for i in range(len(actions_v)):\n",
    "    #    state_action_values[i] = mat[i, actions_v[i]]\n",
    "\n",
    "    next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "    next_state_values[done_mask] = 0.0\n",
    "    next_state_values = next_state_values.detach()\n",
    "\n",
    "    expected_state_action_values = rewards_v + GAMMA * next_state_values\n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "######################################################################\n",
    "\n",
    "#print_flag = True\n",
    "net = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "tgt_net = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "net=net.float()\n",
    "tgt_net=tgt_net.float()\n",
    "writer = SummaryWriter(comment=\"-\" + DEFAULT_ENV_NAME)\n",
    "print(net)\n",
    "\n",
    "buffer = ExperienceBuffer(BUFFER_SIZE)\n",
    "agent = Agent(env, buffer)\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "total_rewards = []\n",
    "frame_idx = 0\n",
    "ts_frame = 0\n",
    "ts = time.time()\n",
    "start_time = ts\n",
    "best_mean_reward = None\n",
    "\n",
    "\n",
    "while True:\n",
    "    frame_idx += 1\n",
    "    # Epsilon decay linear or exponentially\n",
    "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_STEPS)\n",
    "    #epsilon = EPSILON_FINAL + (EPSILON_START-EPSILON_FINAL)*np.exp(-frame_idx/EPSILON_STEPS)\n",
    "\n",
    "    done_reward = agent.play_step(net, epsilon, device=\"gpu\")\n",
    "    if done_reward is not None:\n",
    "        total_rewards.append(done_reward)\n",
    "        #speed = 1 #(frame_idx - ts_frame) / (time.time() - ts)\n",
    "        #ts_frame = frame_idx\n",
    "        #ts = time.time()\n",
    "        mean_reward = np.mean(total_rewards[-100:])\n",
    "        print(\"%d: done %d games, mean reward %.3f, eps %.2f\" % (\n",
    "            frame_idx, len(total_rewards), mean_reward, epsilon))\n",
    "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "        writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "        writer.add_scalar(\"reward\", done_reward, frame_idx)\n",
    "        #if best_mean_reward is None or best_mean_reward < mean_reward:\n",
    "        #    torch.save(net.state_dict(), DEFAULT_ENV_NAME + \"-best.dat\")\n",
    "        #    if best_mean_reward is not None:\n",
    "        #        print(\"Best mean reward updated %.3f -> %.3f, model saved\" % (best_mean_reward, mean_reward))\n",
    "        #    best_mean_reward = mean_reward\n",
    "        if mean_reward > MEAN_REWARD_BOUND:\n",
    "            print(\"Solved in %d frames!\" % frame_idx)\n",
    "            print('Solved in {} seconds'.format(time.time()-start_time))\n",
    "            break\n",
    "\n",
    "    if len(buffer) < REPLAY_START_SIZE:\n",
    "        continue\n",
    "\n",
    "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "        tgt_net.load_state_dict(net.state_dict())\n",
    "        print(\"Target net update, {}\".format(frame_idx))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    loss_t = calc_loss(batch, net, tgt_net, device=\"gpu\")\n",
    "    loss_t.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Deep Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def testDeepQ(output):\n",
    "    ''' Test Deep Q Agent and print out average of 100 episodes (if output == False)\n",
    "    or print one trajectory into file (if output == True)\n",
    "    '''\n",
    "    mean_reward = 0\n",
    "    step_count = 0\n",
    "\n",
    "    if output == True:\n",
    "        standard_out = sys.stdout\n",
    "        sys.stdout = open('Render.txt', 'w')\n",
    "\n",
    "        tries = 1\n",
    "    else:\n",
    "        tries = 100\n",
    "    i = 0\n",
    "    if output == True:\n",
    "        agent._reset(render=True)\n",
    "    else:\n",
    "        agent._reset()\n",
    "    while i < tries:\n",
    "        step_count += 1\n",
    "        if output == True:\n",
    "            reward = agent.play_step(net, device=\"gpu\", render=True)\n",
    "        else:\n",
    "            reward = agent.play_step(net, device=\"gpu\")\n",
    "        \n",
    "        if reward is not None:\n",
    "            mean_reward += reward\n",
    "            i += 1\n",
    "    \n",
    "    mean_reward /= tries\n",
    "    if output == True:\n",
    "        sys.stdout = standard_out\n",
    "        print('Reward: {} after {} steps'.format(mean_reward, step_count))\n",
    "    else:\n",
    "        print('Average reward: {}'.format(mean_reward))\n",
    "\n",
    "    agent.close()\n",
    "\n",
    "testDeepQ(output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = standard_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Version with ptan <a class=\"anchor\" id=\"Ptan\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "MEAN_REWARD_BOUND = 0.6\n",
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_STOP = 0.02\n",
    "EPSILON_STEPS = 5000\n",
    "\n",
    "REPLAY_BUFFER = 50000\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "\n",
    "def calc_target(net, local_reward, next_state):\n",
    "    if next_state is None:\n",
    "        return local_reward\n",
    "    state_v = torch.tensor([next_state], dtype=torch.float32)\n",
    "    next_q_v = net(state_v)\n",
    "    best_q = next_q_v.max(dim=1)[0].item()\n",
    "    return local_reward + GAMMA * best_q\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "    writer = SummaryWriter(comment=\"-FrozenLakev0_ptan\")\n",
    "    net = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "    print(net)\n",
    "\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=EPSILON_START)\n",
    "    agent = ptan.agent.DQNAgent(net, selector)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA, steps_count=1)\n",
    "    replay_buffer = ptan.experience.ExperienceReplayBuffer(exp_source, REPLAY_BUFFER)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "    mse_loss = nn.MSELoss()\n",
    "\n",
    "    total_rewards = []\n",
    "    step_idx = 0\n",
    "    done_episodes = 0\n",
    "\n",
    "    while True:\n",
    "        step_idx += 1\n",
    "        selector.epsilon = max(EPSILON_STOP, EPSILON_START - step_idx / EPSILON_STEPS)\n",
    "        replay_buffer.populate(1)\n",
    "\n",
    "        if len(replay_buffer) < BATCH_SIZE:\n",
    "            continue\n",
    "\n",
    "        # sample batch\n",
    "        batch = replay_buffer.sample(BATCH_SIZE)\n",
    "        batch_states = [exp.state for exp in batch]\n",
    "        batch_actions = [exp.action for exp in batch]\n",
    "        batch_targets = [calc_target(net, exp.reward, exp.last_state) for exp in batch]\n",
    "        # train\n",
    "        optimizer.zero_grad()\n",
    "        states_v = torch.FloatTensor(batch_states)\n",
    "        net_q_v = net(states_v)\n",
    "\n",
    "        target_q = net_q_v.data.numpy().copy()\n",
    "        target_q[range(BATCH_SIZE), batch_actions] = batch_targets\n",
    "        target_q_v = torch.tensor(target_q)\n",
    "        loss_v = mse_loss(net_q_v, target_q_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # handle new rewards\n",
    "        new_rewards = exp_source.pop_total_rewards()\n",
    "        if new_rewards:\n",
    "            done_episodes += 1\n",
    "            reward = new_rewards[0]\n",
    "            total_rewards.append(reward)\n",
    "            mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "            print(\"%d: reward: %6.2f, mean_100: %6.2f, epsilon: %.2f, episodes: %d\" % (\n",
    "                step_idx, reward, mean_rewards, selector.epsilon, done_episodes))\n",
    "            writer.add_scalar(\"reward\", reward, step_idx)\n",
    "            writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "            writer.add_scalar(\"epsilon\", selector.epsilon, step_idx)\n",
    "            if mean_rewards > MEAN_REWARD_BOUND:\n",
    "                print(\"Solved in %d steps and %d episodes!\" % (step_idx, done_episodes))\n",
    "                break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Version with ptan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def testDeepQ_ptan(output):\n",
    "    ''' Test Deep Q Agent and print out average of 100 episodes (if output == False)\n",
    "    or print one trajectory into file (if output == True)\n",
    "    '''\n",
    "    mean_reward = 0\n",
    "\n",
    "    if output == True:\n",
    "        standard_out = sys.stdout\n",
    "        sys.stdout = open('Render.txt', 'w')\n",
    "\n",
    "        tries = 1\n",
    "    else:\n",
    "        tries = 100\n",
    "\n",
    "    state = env.reset()\n",
    "    if output == True:\n",
    "        env.render()\n",
    "\n",
    "    i = 0\n",
    "    steps = 0\n",
    "    while i < tries:\n",
    "        state_v = torch.FloatTensor(state)\n",
    "        action = torch.max(net(state_v), dim=0)[1]\n",
    "        action_v = action.item()\n",
    "        state, reward, done, _ = env.step(action_v)\n",
    "        steps += 1\n",
    "        if output == True:\n",
    "            env.render()\n",
    "        if done == True:\n",
    "            mean_reward += reward\n",
    "            i +=1\n",
    "            state = env.reset()\n",
    "\n",
    "    mean_reward /= tries\n",
    "    if output == True:\n",
    "        sys.stdout = standard_out\n",
    "        print('Reward: {} after {} steps'.format(mean_reward, steps))\n",
    "    else:\n",
    "        print('Average reward: {}'.format(mean_reward))\n",
    "\n",
    "    env.close()\n",
    "\n",
    "testDeepQ_ptan(output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
